{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from dataclasses import dataclass\n",
    "from scipy.spatial.transform import Rotation as R  # rotation axis ??? left-hand / clockwise\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path as path\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Get 3D label and bounding boxes (from two images) for each scene\n",
    "'''\n",
    "def get_location(captures):\n",
    "    # coeff = np.array([-0.00389254, 0.49512566, 0.13212298])\n",
    "\n",
    "    def _find(l, s):\n",
    "        for elem in l:\n",
    "            if elem['id'] == s or elem['id'] == f'{s}_0':\n",
    "                values = elem['values']\n",
    "                for i, value in enumerate(values):\n",
    "                    if value['labelName'] == 'human':\n",
    "                        break\n",
    "                return values[i]\n",
    "    \n",
    "    def _get_bbox(view):\n",
    "        '''\n",
    "            For each view, extract the 2D bounding box and\n",
    "        '''\n",
    "        bbox_3d, bbox_2d = _find(view['annotations'], 'bounding box 3D'), _find(view['annotations'], 'bounding box')\n",
    "        \n",
    "        # refine 3D location of the object\n",
    "        euler = R.from_quat(bbox_3d.get('rotation')).as_matrix()\n",
    "        obj_location = view.get('position') + np.dot(bbox_3d['translation'], euler) # - bbox_3d['size']*np.array([-0.00389254, 0.49512566, 0.13212298])\n",
    "        # pre-process 2D bbox\n",
    "        bbox = {\n",
    "            'center': np.array(bbox_2d['origin']) + np.array(bbox_2d['dimension'])/2,\n",
    "            'size': np.array(bbox_2d['dimension'])/2,\n",
    "        }\n",
    "\n",
    "        return obj_location, bbox\n",
    "\n",
    "    angles = [\n",
    "        [10, 245, 0],\n",
    "        [7, 120, 0],\n",
    "    ]\n",
    "    bboxes, cameras = [], []\n",
    "    for i, view in enumerate(captures):  # per view\n",
    "        obj_location, bbox_2d = _get_bbox(view)\n",
    "\n",
    "        bboxes.append(bbox_2d)\n",
    "        cameras.append({\n",
    "            'filename': view['filename'],\n",
    "            'position': view['position'],\n",
    "            'rotation': angles[i],\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        '3D_location': obj_location,\n",
    "        'bboxes': bboxes,\n",
    "        'cameras': cameras,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Camera:\n",
    "    position: np.ndarray#=field(default_factory=np.array([0., 0., 0.]))\n",
    "    # angle: np.ndarray#=field(default_factory=np.array([0., 1., 0.]))  # angle in degrees\n",
    "    angle: np.ndarray#=field(default_factory=np.array([0., 1., 0.]))  # angle in quaternion\n",
    "\n",
    "    focal_length: float=20.\n",
    "    resolution: np.ndarray=np.array([3840, 2160])\n",
    "    sensor_size: np.ndarray=np.array([30, 30])\n",
    "    fov: float=73.73979\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print('info', self.position, self.angle)\n",
    "        pitch, yaw, roll = self.angle  # R_x, R_y, R_z\n",
    "\n",
    "        sin_yaw, cos_yaw = np.sin(np.deg2rad(yaw)), np.cos(np.deg2rad(yaw))\n",
    "        sin_pitch, cos_pitch = np.sin(np.deg2rad(pitch)), np.cos(np.deg2rad(pitch))\n",
    "        sin_roll, cos_roll = np.sin(np.deg2rad(roll)), np.cos(np.deg2rad(roll))\n",
    "        \n",
    "        self.rotation_yaw = np.array([\n",
    "            [cos_yaw, 0, -sin_yaw],\n",
    "            [0, 1, 0],\n",
    "            [sin_yaw, 0, cos_yaw],       \n",
    "        ])\n",
    "        self.rotation_pitch = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, cos_pitch, -sin_pitch],\n",
    "            [0, sin_pitch, cos_pitch],\n",
    "        ])\n",
    "        self.rotation_roll = np.array([\n",
    "            [cos_roll, -sin_roll, 0],\n",
    "            [sin_roll, cos_roll, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        self.rotation = self.rotation_yaw @ self.rotation_pitch @ self.rotation_roll\n",
    "\n",
    "        self.intrinsic = np.array([\n",
    "            [self.focal_length*self.resolution[0]/self.sensor_size[0], 0, self.resolution[0]/2],\n",
    "            [0, self.focal_length*self.resolution[1]/self.sensor_size[1], self.resolution[1]/2],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    def pixel2ray(self, pixel):\n",
    "        pixel = np.append(pixel, np.zeros((pixel.shape[0], 1)) + 1, axis=-1)\n",
    "        camera_coor = pixel @ np.linalg.inv(self.intrinsic)\n",
    "        world_coor = (camera_coor @ self.rotation)\n",
    "\n",
    "        vector = world_coor - np.array(self.position)\n",
    "        directional_vector = vector / np.linalg.norm(vector)\n",
    "        \n",
    "        return [Line(origin=coor, direction=vec) for coor, vec in zip(world_coor, directional_vector)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Line:\n",
    "    origin: np.ndarray\n",
    "    direction: np.ndarray\n",
    "\n",
    "\n",
    "def find_points(line_a: Line, line_b: Line):\n",
    "    n = np.cross(line_a.direction, line_b.direction)\n",
    "    d = np.abs(np.dot(n, line_a.origin - line_b.origin)) / np.linalg.norm(n)\n",
    "    \n",
    "    t_a = np.dot(np.cross(line_b.direction, n), (line_b.origin - line_a.origin)) / np.dot(n, n)\n",
    "    t_b = np.dot(np.cross(line_a.direction, n), (line_b.origin - line_a.origin)) / np.dot(n, n)\n",
    "\n",
    "    p_a = line_a.origin + t_a * line_a.direction\n",
    "    p_b = line_b.origin + t_b * line_b.direction\n",
    "\n",
    "    return (p_a + p_b) / 2\n",
    "\n",
    "colors = ['red', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.046828758, -0.8401821, 0.0735064447, -0.535254955]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(json_file)\n\u001b[1;32m      8\u001b[0m captures \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mcaptures\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m output \u001b[39m=\u001b[39m get_location(captures)\n\u001b[1;32m     10\u001b[0m bboxes, cameras \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mbboxes\u001b[39m\u001b[39m'\u001b[39m], output[\u001b[39m'\u001b[39m\u001b[39mcameras\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(bboxes)\n",
      "Cell \u001b[0;32mIn[14], line 63\u001b[0m, in \u001b[0;36mget_location\u001b[0;34m(captures)\u001b[0m\n\u001b[1;32m     61\u001b[0m bboxes, cameras \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m i, view \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(captures):  \u001b[39m# per view\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     obj_location, bbox_2d \u001b[39m=\u001b[39m _get_bbox(view)\n\u001b[1;32m     65\u001b[0m     bboxes\u001b[39m.\u001b[39mappend(bbox_2d)\n\u001b[1;32m     66\u001b[0m     cameras\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     67\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m: view[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     68\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m: view[\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     69\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mrotation\u001b[39m\u001b[39m'\u001b[39m: angles[i],\n\u001b[1;32m     70\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mget_location.<locals>._get_bbox\u001b[0;34m(view)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# refine 3D location of the object\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# euler = R.from_quat(bbox_3d.get('rotation')).as_matrix()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(bbox_3d\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrotation\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m pitch, yaw, roll \u001b[39m=\u001b[39m bbox_3d\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrotation\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# R_x, R_y, R_z\u001b[39;00m\n\u001b[1;32m     27\u001b[0m sin_yaw, cos_yaw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msin(np\u001b[39m.\u001b[39mdeg2rad(yaw)), np\u001b[39m.\u001b[39mcos(np\u001b[39m.\u001b[39mdeg2rad(yaw))\n\u001b[1;32m     28\u001b[0m sin_pitch, cos_pitch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msin(np\u001b[39m.\u001b[39mdeg2rad(pitch)), np\u001b[39m.\u001b[39mcos(np\u001b[39m.\u001b[39mdeg2rad(pitch))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "folder_path = path.joinpath(path.cwd(), 'solo')\n",
    "\n",
    "for i in range(10):  # per scene\n",
    "    # Extract 3D label and bboxes of each image from a scene\n",
    "    file_path = path.joinpath(folder_path, f'sequence.{i}')\n",
    "    with open(path.joinpath(file_path, 'step0.frame_data.json')) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "    output = get_location(captures)\n",
    "    bboxes, cameras = output['bboxes'], output['cameras']\n",
    "\n",
    "    print(bboxes)\n",
    "    print(cameras)\n",
    "\n",
    "    cameras_ = [Camera(position=camera['position'], angle=camera['rotation']) for camera in cameras]\n",
    "\n",
    "    lines = []\n",
    "    for i, (bbox, camera) in enumerate(zip(output.get('bboxes'), cameras_)):\n",
    "        pixels = np.array([\n",
    "            bbox['center']\n",
    "        ])\n",
    "        rays = camera.pixel2ray(pixels)\n",
    "        stack_origin = np.stack([camera.position]*pixels.shape[0])\n",
    "        lines.append(rays[-1])\n",
    "\n",
    "    point = find_points(*lines)\n",
    "    print('lpm', np.sum((point - output['3D_location'])**2))\n",
    "    print(point)\n",
    "    print(output['3D_location'])\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a94878f375e66513a9f58a51086fe49a2b3775db248e5bb5453de8c77189b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
