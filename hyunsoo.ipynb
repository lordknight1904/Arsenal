{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from dataclasses import dataclass, field\n",
    "from scipy.spatial.transform import Rotation as R  # rotation axis ??? left-hand / clockwise\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path as path\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Get 3D label and bounding boxes (from two images) for each scene\n",
    "'''\n",
    "def get_location(captures):\n",
    "    # coeff = np.array([-0.00389254, 0.49512566, 0.13212298])\n",
    "\n",
    "    def _find(l, s):\n",
    "        for elem in l:\n",
    "            if elem['id'] == s or elem['id'] == f'{s}_0':\n",
    "                values = elem['values']\n",
    "                for i, value in enumerate(values):\n",
    "                    if value['labelName'] == 'human':\n",
    "                        break\n",
    "                else:\n",
    "                    return None\n",
    "                return values[i]\n",
    "    \n",
    "    def _get_bbox(view):\n",
    "        '''\n",
    "            For each view, extract the 2D bounding box and\n",
    "        '''\n",
    "        bbox_3d, bbox_2d = _find(view['annotations'], 'bounding box 3D'), _find(view['annotations'], 'bounding box')\n",
    "\n",
    "        q = R.from_quat(bbox_3d.get('rotation'))\n",
    "\n",
    "        obj_location = q.apply(bbox_3d.get('translation'), inverse=True) + view.get('position')\n",
    "        # pre-process 2D bbox\n",
    "        bbox = {\n",
    "            'center': np.array(bbox_2d['origin']) + np.array(bbox_2d['dimension'])/2,\n",
    "            'size': np.array(bbox_2d['dimension'])/2,\n",
    "        }\n",
    "        return obj_location, bbox\n",
    "\n",
    "    # angles = [\n",
    "    #     [10, 245, 0],\n",
    "    #     [7, 120, 0],\n",
    "    # ]\n",
    "    angles = [\n",
    "        [0, 225, 0],\n",
    "        [0, 135, 0],\n",
    "    ]\n",
    "    bboxes, cameras = [], []\n",
    "    for i, view in enumerate(captures):  # per view\n",
    "        obj_location, bbox_2d = _get_bbox(view)\n",
    "\n",
    "        bboxes.append(bbox_2d)\n",
    "        cameras.append({\n",
    "            'filename': view['filename'],\n",
    "            'position': view['position'],\n",
    "            'rotation': angles[i],\n",
    "            'quaternion': view.get('rotation'),\n",
    "            'intrinsic': view.get('matrix')\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        '3D_location': obj_location,\n",
    "        'bboxes': bboxes,\n",
    "        'cameras': cameras,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Camera:\n",
    "    position: np.ndarray#=field(default_factory=np.array([0., 0., 0.]))\n",
    "    angle: np.ndarray#=field(default_factory=np.array([0., 1., 0.]))  # angle in degree\n",
    "    intrinsic_: np.ndarray#=field(default_factory=np.array([0., 1., 0.]))  # angle in degree\n",
    "    quaternion: np.ndarray=field(default_factory=np.array([1., 0., 0., 0.]))  # angle in quaternion\n",
    "\n",
    "    focal_length: float=20.\n",
    "    resolution: np.ndarray=np.array([3840, 2160])\n",
    "    sensor_size: np.ndarray=np.array([30, 30])\n",
    "    fov: float=73.73979\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # print('info', self.position, self.angle, self.quaternion)\n",
    "        pitch, yaw, roll = self.angle  # R_x, R_y, R_z\n",
    "\n",
    "        sin_yaw, cos_yaw = np.sin(np.deg2rad(yaw)), np.cos(np.deg2rad(yaw))\n",
    "        sin_pitch, cos_pitch = np.sin(np.deg2rad(pitch)), np.cos(np.deg2rad(pitch))\n",
    "        sin_roll, cos_roll = np.sin(np.deg2rad(roll)), np.cos(np.deg2rad(roll))\n",
    "        \n",
    "        self.rotation_yaw = np.array([\n",
    "            [cos_yaw, 0, -sin_yaw],\n",
    "            [0, 1, 0],\n",
    "            [sin_yaw, 0, cos_yaw],       \n",
    "        ])\n",
    "        self.rotation_pitch = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, cos_pitch, -sin_pitch],\n",
    "            [0, sin_pitch, cos_pitch],\n",
    "        ])\n",
    "        self.rotation_roll = np.array([\n",
    "            [cos_roll, -sin_roll, 0],\n",
    "            [sin_roll, cos_roll, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        # self.rotation = self.rotation_yaw @ self.rotation_pitch @ self.rotation_roll\n",
    "        self.rotation = self.rotation_roll @ self.rotation_pitch @ self.rotation_yaw\n",
    "\n",
    "        self.intrinsic = np.array([\n",
    "            [self.focal_length*self.resolution[0]/self.sensor_size[0], 0, self.resolution[0]/2],\n",
    "            [0, self.focal_length*self.resolution[1]/self.sensor_size[1], self.resolution[1]/2],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        self.intrinsic_ = np.array(self.intrinsic_).reshape((3, 3))\n",
    "\n",
    "    def pixel2ray(self, pixel):\n",
    "        pixel = np.append(pixel, np.zeros((pixel.shape[0], 1)) + 1, axis=-1)\n",
    "        # camera_coor = pixel @ self.intrinsic_\n",
    "        camera_coor = pixel @ np.linalg.inv(self.intrinsic)\n",
    "        camera_coor = camera_coor / camera_coor[:, -1]\n",
    "        # camera_coor = pixel @ np.linalg.inv(self.intrinsic_)\n",
    "        world_coor = (camera_coor @ self.rotation)\n",
    "        vector = world_coor - np.array(self.position)\n",
    "        directional_vector = vector / np.linalg.norm(vector)\n",
    "\n",
    "        # print(self.quaternion)\n",
    "        # q = R.from_quat(self.quaternion)\n",
    "        # world_coor = q.apply(camera_coor, inverse=True) - self.position\n",
    "        # directional_vector = world_coor / np.linalg.norm(world_coor)\n",
    "        \n",
    "        return [Line(origin=coor, direction=vec) for coor, vec in zip(world_coor, directional_vector)]\n",
    "        \n",
    "@dataclass\n",
    "class Line:\n",
    "    origin: np.ndarray\n",
    "    direction: np.ndarray\n",
    "\n",
    "\n",
    "def find_points(line_a: Line, line_b: Line):\n",
    "    n = np.cross(line_a.direction, line_b.direction)\n",
    "    d = np.abs(np.dot(n, line_a.origin - line_b.origin)) / np.linalg.norm(n)\n",
    "    \n",
    "    t_a = np.dot(np.cross(line_b.direction, n), (line_b.origin - line_a.origin)) / np.dot(n, n)\n",
    "    t_b = np.dot(np.cross(line_a.direction, n), (line_b.origin - line_a.origin)) / np.dot(n, n)\n",
    "\n",
    "    p_a = line_a.origin + t_a * line_a.direction\n",
    "    p_b = line_b.origin + t_b * line_b.direction\n",
    "\n",
    "    return (p_a + p_b) / 2\n",
    "\n",
    "colors = ['red', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0\n",
      "[0.15049528 0.37517559 0.45076814]\n",
      "[ 0.04815929  1.11557293 -1.53010453]\n",
      "lpm 4.482517429456927\n",
      "------------------------------\n",
      "sample 1\n",
      "[0.15049208 0.37519587 0.45065092]\n",
      "[-0.32352058  1.11557269  3.04011358]\n",
      "lpm 7.478162685552228\n",
      "------------------------------\n",
      "sample 2\n",
      "[0.15049078 0.37517599 0.45073344]\n",
      "[0.32269124 1.11557293 0.21541381]\n",
      "lpm 0.6332159538254851\n",
      "------------------------------\n",
      "sample 3\n",
      "[0.15044741 0.37511251 0.45071041]\n",
      "[1.9712949  1.11557293 2.46421274]\n",
      "lpm 7.917958857848313\n",
      "------------------------------\n",
      "sample 4\n",
      "[0.15050851 0.37520949 0.45070796]\n",
      "[-0.78690091  1.11557293  0.36978478]\n",
      "lpm 1.4334230116463162\n",
      "------------------------------\n",
      "sample 5\n",
      "[0.15050319 0.3751933  0.450744  ]\n",
      "[-0.38637322  1.11557269 -0.971902  ]\n",
      "lpm 2.8603195873491516\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(json_file)\n\u001b[1;32m      9\u001b[0m captures \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mcaptures\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m output \u001b[39m=\u001b[39m get_location(captures)\n\u001b[1;32m     11\u001b[0m bboxes, cameras \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mbboxes\u001b[39m\u001b[39m'\u001b[39m], output[\u001b[39m'\u001b[39m\u001b[39mcameras\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m cameras_ \u001b[39m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     Camera(position\u001b[39m=\u001b[39mcamera[\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m], angle\u001b[39m=\u001b[39mcamera[\u001b[39m'\u001b[39m\u001b[39mrotation\u001b[39m\u001b[39m'\u001b[39m], intrinsic_\u001b[39m=\u001b[39mcamera[\u001b[39m'\u001b[39m\u001b[39mintrinsic\u001b[39m\u001b[39m'\u001b[39m], quaternion\u001b[39m=\u001b[39mcamera[\u001b[39m'\u001b[39m\u001b[39mquaternion\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m camera \u001b[39min\u001b[39;00m output[\u001b[39m'\u001b[39m\u001b[39mcameras\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mget_location\u001b[0;34m(captures)\u001b[0m\n\u001b[1;32m     42\u001b[0m bboxes, cameras \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m i, view \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(captures):  \u001b[39m# per view\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     obj_location, bbox_2d \u001b[39m=\u001b[39m _get_bbox(view)\n\u001b[1;32m     46\u001b[0m     bboxes\u001b[39m.\u001b[39mappend(bbox_2d)\n\u001b[1;32m     47\u001b[0m     cameras\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m: view[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     49\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m: view[\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mintrinsic\u001b[39m\u001b[39m'\u001b[39m: view\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmatrix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m, in \u001b[0;36mget_location.<locals>._get_bbox\u001b[0;34m(view)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_bbox\u001b[39m(view):\n\u001b[1;32m     19\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m        For each view, extract the 2D bounding box and\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     bbox_3d, bbox_2d \u001b[39m=\u001b[39m _find(view[\u001b[39m'\u001b[39;49m\u001b[39mannotations\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39mbounding box 3D\u001b[39;49m\u001b[39m'\u001b[39;49m), _find(view[\u001b[39m'\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mbounding box\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m     q \u001b[39m=\u001b[39m R\u001b[39m.\u001b[39mfrom_quat(bbox_3d\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrotation\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     26\u001b[0m     obj_location \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mapply(bbox_3d\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m'\u001b[39m), inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m view\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mget_location.<locals>._find\u001b[0;34m(l, s)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m l:\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m elem[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m s \u001b[39mor\u001b[39;00m elem[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m_0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m         values \u001b[39m=\u001b[39m elem[\u001b[39m'\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     11\u001b[0m         \u001b[39mfor\u001b[39;00m i, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(values):\n\u001b[1;32m     12\u001b[0m             \u001b[39mif\u001b[39;00m value[\u001b[39m'\u001b[39m\u001b[39mlabelName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'values'"
     ]
    }
   ],
   "source": [
    "folder_path = path.joinpath(path.cwd(), 'solo_9')\n",
    "\n",
    "mse = []\n",
    "for i in range(10):  # per scene\n",
    "    # Extract 3D label and bboxes of each image from a scene\n",
    "    file_path = path.joinpath(folder_path, f'sequence.{i}')\n",
    "    with open(path.joinpath(file_path, 'step0.frame_data.json')) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "    output = get_location(captures)\n",
    "    bboxes, cameras = output['bboxes'], output['cameras']\n",
    "\n",
    "    cameras_ = [\n",
    "        Camera(position=camera['position'], angle=camera['rotation'], intrinsic_=camera['intrinsic'], quaternion=camera['quaternion'])\n",
    "        for camera in output['cameras']\n",
    "    ]\n",
    "\n",
    "    lines = []\n",
    "    for j, (bbox, camera) in enumerate(zip(output.get('bboxes'), cameras_)):\n",
    "        pixels = np.array([\n",
    "            bbox['center']\n",
    "        ])\n",
    "        rays = camera.pixel2ray(pixels)\n",
    "        stack_origin = np.stack([camera.position]*pixels.shape[0])\n",
    "        lines.append(rays[-1])\n",
    "\n",
    "    point = find_points(*lines)\n",
    "    m = np.sum((point - output['3D_location'])**2)\n",
    "    print(f'sample {i}')\n",
    "    print(point)\n",
    "    print(output['3D_location'])\n",
    "    print('lpm', m)\n",
    "    print('-'*30)\n",
    "    mse.append(m)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a94878f375e66513a9f58a51086fe49a2b3775db248e5bb5453de8c77189b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
