This module provides basic components of Transformers.


Basic components of a Transformer.
First and foremost component is the Self-attention module who takes input as a sequence of token and calculate pairwise attention scores.
