{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autobot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_view = 3\n",
    "h, w = 256, 256\n",
    "\n",
    "imgs = torch.rand((batch_size, num_view, 3, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 128])\n"
     ]
    }
   ],
   "source": [
    "emb_layer = autobot.MultiViewEmbedding(emb_dim=128, patch_size=(16, 16))\n",
    "emb = emb_layer(imgs)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 128])\n",
      "query: torch.Size([4, 256, 128]) torch.Size([4, 256, 8])\n",
      "------------------------------\n",
      "key, value: torch.Size([4, 2, 256, 128]) torch.Size([4, 2, 256, 8]) torch.Size([4, 2, 256, 16])\n",
      "torch.Size([4, 1, 256, 8])\n",
      "torch.Size([4, 2, 256, 256])\n",
      "torch.Size([4, 2, 256, 256])\n",
      "v torch.Size([4, 2, 256, 16])\n",
      "o torch.Size([4, 256, 16])\n",
      "torch.Size([4, 256, 16])\n"
     ]
    }
   ],
   "source": [
    "class MultiViewAttention(nn.Module):\n",
    "    '''\n",
    "        MVA searches object in 3D space by computing similarity \n",
    "    '''\n",
    "    def __init__(self,\n",
    "        emb_dim, attn_dim, v_dim, num_heads,\n",
    "        num_view, view_transformation=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.v_dim = v_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        self.value = nn.Linear(emb_dim, v_dim)\n",
    "        self.linear = nn.Linear(v_dim*num_heads, v_dim*num_heads)\n",
    "\n",
    "        self.per_view_mha = nn.ModuleList([\n",
    "            autobot.MultiHeadAttention(emb_dim, attn_dim, v_dim, num_heads)\n",
    "            for _ in range(num_view)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, v, vs):\n",
    "        query = self.query(v)\n",
    "        \n",
    "        print('query:', v.shape, query.shape)\n",
    "        print('-'*30)\n",
    "        key = self.key(vs)\n",
    "        value = self.value(vs)\n",
    "        print('key, value:', vs.shape, key.shape, value.shape)\n",
    "\n",
    "        print(query.unsqueeze(1).shape)\n",
    "        logits = torch.matmul(query.unsqueeze(1), key.transpose(-1, -2))\n",
    "        print(logits.shape)\n",
    "        logits_ = logits.sum(1)\n",
    "        print(logits.shape)\n",
    "        attn_ = logits_.softmax(dim=-1)\n",
    "\n",
    "        attn = logits.softmax(dim=-1)\n",
    "        v = torch.matmul(attn, value)\n",
    "        print('v', v.shape)\n",
    "\n",
    "        o = torch.matmul(attn_, v.sum(1))\n",
    "        print('o', o.shape)\n",
    "\n",
    "        return self.linear(o)\n",
    "\n",
    "\n",
    "mva_layer = MultiViewAttention(emb_dim=128, attn_dim=8, v_dim=16, num_heads=1, num_view=num_view)\n",
    "i = 0\n",
    "print(emb.shape)\n",
    "mva = mva_layer(\n",
    "    emb[:, i, :, :], \n",
    "    torch.cat([\n",
    "        emb[:, :i, :, :],\n",
    "        emb[:, i+1:, :, :]\n",
    "    ], dim=1)  # from i-th view to other views that are not i-th\n",
    ")\n",
    "print(mva.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [[ 282.363047,      0.,          166.21515189],\n",
    "     [   0.,          280.10715905,  108.05494375],\n",
    "     [   0.,            0.,            1.        ]]\n",
    "K = np.array(K)\n",
    "R = np.eye(3)\n",
    "t = np.array([[0],[1.],[0]])\n",
    "P = K.dot(np.hstack((R,t)))\n",
    "C = np.array([0., 0., 1.])\n",
    "p1 = np.array([215, 180, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[282.363047  ,   0.        , 166.21515189,   0.        ],\n",
       "       [  0.        , 280.10715905, 108.05494375, 280.10715905],\n",
       "       [  0.        ,   0.        ,   1.        ,   0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((R,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a94878f375e66513a9f58a51086fe49a2b3775db248e5bb5453de8c77189b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
